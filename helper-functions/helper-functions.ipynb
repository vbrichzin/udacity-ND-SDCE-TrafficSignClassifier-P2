{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Step/Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.3775406687981454\n",
      "Amount of Error:\n",
      "0.1224593312018546\n",
      "Change in Weights:\n",
      "[0.0143892 0.0287784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error * nn_output * (1 - nn_output) * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the \"solution.ipynb\" \n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session as the variable output\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"quiz_solution.ipynb\" \n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    bias = tf.Variable(tf.zeros(n_labels))\n",
    "    return bias\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    output = tf.add(tf.matmul(input, w), b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-c3464672ce23>:16: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Loss: 7.618446350097656\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"quiz_solution.ipynb\" tab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(init)\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 5  7  9]\n",
      " [ 8 10 12]\n",
      " [11 13 15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "u = np.array([1, 2, 3])\n",
    "print(t + u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.ipynb\" \n",
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24965a3f4a45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eea774cfe28e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatches\u001b[0m  \u001b[1;31m# Helper function created in Mini-batching section\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "## Problem 1\n",
    "Implement the Min-Max scaling function ($X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}$) with the parameters:\n",
    "\n",
    "$X_{\\min }=0$\n",
    "\n",
    "$X_{\\max }=255$\n",
    "\n",
    "$a=0.1$\n",
    "\n",
    "$b=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "- Use [tf.placeholder()](https://www.tensorflow.org/api_docs/python/io_ops.html#placeholder) for `features` and `labels` since they are the inputs to the model.\n",
    "- Any math operations must have the same type on both sides of the operator.  The weights are float32, so the `features` and `labels` must also be float32.\n",
    "- Use [tf.Variable()](https://www.tensorflow.org/api_docs/python/state_ops.html#Variable) to allow `weights` and `biases` to be modified.\n",
    "- The `weights` must be the dimensions of features by labels.  The number of features is the size of the image, 28*28=784.  The size of labels is 10.\n",
    "- The `biases` must be the dimensions of the labels, which is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = 784\n",
    "labels_count = 10\n",
    "\n",
    "# Problem 2 - Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Problem 2 - Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\n",
    "biases = tf.Variable(tf.zeros(labels_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Configuration 1\n",
    "* **Epochs:** 1\n",
    "* **Batch Size:** 50\n",
    "* **Learning Rate:** 0.01\n",
    "\n",
    "Configuration 2\n",
    "* **Epochs:** 1\n",
    "* **Batch Size:** 100\n",
    "* **Learning Rate:** 0.1\n",
    "\n",
    "Configuration 3\n",
    "* **Epochs:** 4 or 5\n",
    "* **Batch Size:** 100\n",
    "* **Learning Rate:** 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [ 0.        0.      ]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: save and print session results on variable output\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(logits)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-0.38894972 -0.13214019 -0.65277857]\n",
      " [ 1.3642797   0.08857285 -1.5069958 ]]\n",
      "Bias:\n",
      "[ 0.64462554 -0.1218202  -1.0637972 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-7d8b7f984ace>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-7d8b7f984ace>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    saver.restore(sess, save_file)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f3a650c27404>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-12-f3a650c27404>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation Accuracy: 0.03799999877810478\n",
      "Epoch 10  - Validation Accuracy: 0.17520000040531158\n",
      "Epoch 20  - Validation Accuracy: 0.3352000117301941\n",
      "Epoch 30  - Validation Accuracy: 0.448199987411499\n",
      "Epoch 40  - Validation Accuracy: 0.5311999917030334\n",
      "Epoch 50  - Validation Accuracy: 0.5861999988555908\n",
      "Epoch 60  - Validation Accuracy: 0.626800000667572\n",
      "Epoch 70  - Validation Accuracy: 0.6557999849319458\n",
      "Epoch 80  - Validation Accuracy: 0.676800012588501\n",
      "Epoch 90  - Validation Accuracy: 0.6991999745368958\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n",
      "Test Accuracy: 0.71670001745224\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: weights_0:0\n",
      "Save Bias: bias_0:0\n",
      "Load Weights: weights_0:0\n",
      "Load Bias: bias_0:0\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Loaded Weights and Bias successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [ 0.11200001  0.6720001 ]\n",
      " [ 4.72       28.32      ]]\n"
     ]
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# set random seed\n",
    "tf.set_random_seed(123456)\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: save and print session results as variable named \"output\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(logits, feed_dict={keep_prob: 0.5})\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'SAME'\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-29c269cc31e8>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-5ff0f3be3dc1>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch  1, Batch   1 -Loss: 65757.2344 Validation Accuracy: 0.082031\n",
      "Epoch  1, Batch   2 -Loss: 48201.2031 Validation Accuracy: 0.078125\n",
      "Epoch  1, Batch   3 -Loss: 39486.7930 Validation Accuracy: 0.066406\n",
      "Epoch  1, Batch   4 -Loss: 42144.4922 Validation Accuracy: 0.074219\n",
      "Epoch  1, Batch   5 -Loss: 34611.8203 Validation Accuracy: 0.066406\n",
      "Epoch  1, Batch   6 -Loss: 29850.3633 Validation Accuracy: 0.093750\n",
      "Epoch  1, Batch   7 -Loss: 28294.9590 Validation Accuracy: 0.082031\n",
      "Epoch  1, Batch   8 -Loss: 25357.3438 Validation Accuracy: 0.097656\n",
      "Epoch  1, Batch   9 -Loss: 23640.8672 Validation Accuracy: 0.105469\n",
      "Epoch  1, Batch  10 -Loss: 22469.2070 Validation Accuracy: 0.113281\n",
      "Epoch  1, Batch  11 -Loss: 21387.7734 Validation Accuracy: 0.125000\n",
      "Epoch  1, Batch  12 -Loss: 20623.0898 Validation Accuracy: 0.132812\n",
      "Epoch  1, Batch  13 -Loss: 18533.8359 Validation Accuracy: 0.140625\n",
      "Epoch  1, Batch  14 -Loss: 16500.4219 Validation Accuracy: 0.164062\n",
      "Epoch  1, Batch  15 -Loss: 17432.2109 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  16 -Loss: 19006.7031 Validation Accuracy: 0.152344\n",
      "Epoch  1, Batch  17 -Loss: 17061.7422 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  18 -Loss: 17176.6680 Validation Accuracy: 0.199219\n",
      "Epoch  1, Batch  19 -Loss: 15916.1641 Validation Accuracy: 0.199219\n",
      "Epoch  1, Batch  20 -Loss: 13282.1934 Validation Accuracy: 0.214844\n",
      "Epoch  1, Batch  21 -Loss: 17664.6816 Validation Accuracy: 0.242188\n",
      "Epoch  1, Batch  22 -Loss: 14554.5332 Validation Accuracy: 0.238281\n",
      "Epoch  1, Batch  23 -Loss: 15361.8623 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  24 -Loss: 14028.6270 Validation Accuracy: 0.261719\n",
      "Epoch  1, Batch  25 -Loss: 12455.4727 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  26 -Loss: 14838.6719 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  27 -Loss: 13966.9238 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  28 -Loss: 14393.7861 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  29 -Loss: 10907.2354 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  30 -Loss: 13329.5059 Validation Accuracy: 0.261719\n",
      "Epoch  1, Batch  31 -Loss: 11250.1426 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  32 -Loss: 11224.2754 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  33 -Loss: 11675.5322 Validation Accuracy: 0.320312\n",
      "Epoch  1, Batch  34 -Loss: 12122.8232 Validation Accuracy: 0.332031\n",
      "Epoch  1, Batch  35 -Loss: 11334.1787 Validation Accuracy: 0.332031\n",
      "Epoch  1, Batch  36 -Loss: 11687.2891 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  37 -Loss: 11801.6943 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  38 -Loss: 12615.4248 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  39 -Loss: 12592.6680 Validation Accuracy: 0.332031\n",
      "Epoch  1, Batch  40 -Loss: 12285.9297 Validation Accuracy: 0.332031\n",
      "Epoch  1, Batch  41 -Loss:  8109.0459 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  42 -Loss:  8799.1777 Validation Accuracy: 0.355469\n",
      "Epoch  1, Batch  43 -Loss:  9645.1602 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  44 -Loss: 11437.2109 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  45 -Loss:  9272.3350 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  46 -Loss: 12613.8643 Validation Accuracy: 0.359375\n",
      "Epoch  1, Batch  47 -Loss: 10490.9082 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  48 -Loss: 10092.8789 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  49 -Loss:  9368.5781 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  50 -Loss:  7489.8447 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch  51 -Loss: 10408.5010 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  52 -Loss:  8696.1641 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  53 -Loss:  7149.8867 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  54 -Loss:  9915.1631 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  55 -Loss:  7795.0239 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch  56 -Loss:  8767.7529 Validation Accuracy: 0.375000\n",
      "Epoch  1, Batch  57 -Loss:  8146.6201 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  58 -Loss:  7664.9087 Validation Accuracy: 0.371094\n",
      "Epoch  1, Batch  59 -Loss:  8580.8047 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  60 -Loss:  6955.6816 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  61 -Loss:  7295.9678 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  62 -Loss:  8203.4707 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  63 -Loss:  6719.4912 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  64 -Loss:  8150.2598 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  65 -Loss:  8560.0752 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  66 -Loss:  7187.9302 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  67 -Loss:  7773.3784 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  68 -Loss:  8421.1836 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  69 -Loss:  7192.3018 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  70 -Loss:  5014.2632 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  71 -Loss:  6737.0957 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  72 -Loss:  7353.1934 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  73 -Loss:  6549.6069 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  74 -Loss:  6593.7368 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  75 -Loss:  7287.3901 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  76 -Loss:  6406.9893 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  77 -Loss:  7190.3140 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  78 -Loss:  6289.1631 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  79 -Loss:  6417.3677 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  80 -Loss:  6194.8242 Validation Accuracy: 0.421875\n",
      "Epoch  1, Batch  81 -Loss:  6303.7563 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  82 -Loss:  6482.6035 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  83 -Loss:  6328.1782 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  84 -Loss:  6893.8584 Validation Accuracy: 0.429688\n",
      "Epoch  1, Batch  85 -Loss:  5669.0098 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  86 -Loss:  5644.8789 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  87 -Loss:  5801.3267 Validation Accuracy: 0.445312\n",
      "Epoch  1, Batch  88 -Loss:  6449.1270 Validation Accuracy: 0.429688\n",
      "Epoch  1, Batch  89 -Loss:  5307.9639 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  90 -Loss:  5334.3364 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  91 -Loss:  5840.6157 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  92 -Loss:  4445.2495 Validation Accuracy: 0.468750\n",
      "Epoch  1, Batch  93 -Loss:  4847.3027 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  94 -Loss:  5257.0430 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  95 -Loss:  6694.0933 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch  96 -Loss:  5723.3462 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  97 -Loss:  4707.2373 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  98 -Loss:  6010.8711 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  99 -Loss:  5519.9971 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch 100 -Loss:  4588.9492 Validation Accuracy: 0.445312\n",
      "Epoch  1, Batch 101 -Loss:  4762.1416 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch 102 -Loss:  5655.4404 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch 103 -Loss:  4408.3926 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch 104 -Loss:  4911.3540 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 105 -Loss:  4739.3262 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch 106 -Loss:  5163.8818 Validation Accuracy: 0.468750\n",
      "Epoch  1, Batch 107 -Loss:  6150.1646 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch 108 -Loss:  5164.3062 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch 109 -Loss:  6039.4775 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch 110 -Loss:  4937.1182 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch 111 -Loss:  5264.6475 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch 112 -Loss:  5256.9795 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch 113 -Loss:  5325.6885 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch 114 -Loss:  4053.2407 Validation Accuracy: 0.492188\n",
      "Epoch  1, Batch 115 -Loss:  4718.4463 Validation Accuracy: 0.492188\n",
      "Epoch  1, Batch 116 -Loss:  4638.9478 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 117 -Loss:  4055.7954 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch 118 -Loss:  5022.6943 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 119 -Loss:  4380.6729 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 120 -Loss:  5591.0840 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch 121 -Loss:  4536.3569 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch 122 -Loss:  4309.5688 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 123 -Loss:  4628.2969 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 124 -Loss:  4351.3062 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch 125 -Loss:  3975.1614 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch 126 -Loss:  4616.4595 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 127 -Loss:  4122.6738 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 128 -Loss:  4745.8242 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch 129 -Loss:  4979.7559 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch 130 -Loss:  3871.0005 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 131 -Loss:  4854.0430 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch 132 -Loss:  5173.1123 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 133 -Loss:  4576.4429 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 134 -Loss:  5047.0732 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 135 -Loss:  4941.3643 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 136 -Loss:  4095.7974 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 137 -Loss:  3827.6589 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 138 -Loss:  2951.5977 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 139 -Loss:  3663.1367 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 140 -Loss:  3965.1895 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 141 -Loss:  4080.5369 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch 142 -Loss:  4115.8848 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 143 -Loss:  4130.8516 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch 144 -Loss:  3898.4985 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 145 -Loss:  3827.6807 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 146 -Loss:  3720.0496 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 147 -Loss:  3847.6624 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch 148 -Loss:  4648.0039 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch 149 -Loss:  4242.1841 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch 150 -Loss:  3955.3254 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch 151 -Loss:  5133.0869 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 152 -Loss:  3132.8945 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 153 -Loss:  3424.5881 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch 154 -Loss:  4391.3184 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch 155 -Loss:  3361.2607 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch 156 -Loss:  3380.9617 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch 157 -Loss:  4002.0674 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch 158 -Loss:  3122.9790 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch 159 -Loss:  5122.4355 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 160 -Loss:  3446.2078 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 161 -Loss:  3510.1221 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 162 -Loss:  3512.3403 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch 163 -Loss:  3008.6890 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch 164 -Loss:  3308.3159 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 165 -Loss:  3851.9316 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 166 -Loss:  3937.6790 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch 167 -Loss:  2781.9917 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 168 -Loss:  3146.6592 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 169 -Loss:  3621.3240 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 170 -Loss:  3471.2327 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch 171 -Loss:  3885.7661 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch 172 -Loss:  2906.7512 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 173 -Loss:  3345.2583 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 174 -Loss:  4186.3975 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 175 -Loss:  2910.2910 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 176 -Loss:  2900.2969 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 177 -Loss:  4155.5752 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 178 -Loss:  3919.8752 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 179 -Loss:  2929.5894 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 180 -Loss:  3113.4468 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 181 -Loss:  3347.3760 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 182 -Loss:  3286.7996 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 183 -Loss:  2972.4146 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 184 -Loss:  3543.1484 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 185 -Loss:  3494.4885 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 186 -Loss:  3072.7280 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 187 -Loss:  3047.7612 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 188 -Loss:  3455.6458 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 189 -Loss:  2922.9727 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 190 -Loss:  3710.5090 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 191 -Loss:  2985.9204 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 192 -Loss:  3477.8125 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 193 -Loss:  3570.4490 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 194 -Loss:  2916.3882 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 195 -Loss:  2992.6628 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 196 -Loss:  3524.9641 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 197 -Loss:  2569.9199 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 198 -Loss:  3202.9912 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 199 -Loss:  2706.4114 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 200 -Loss:  3029.2368 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 201 -Loss:  2957.6265 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 202 -Loss:  3048.9199 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 203 -Loss:  3639.0281 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 204 -Loss:  2909.5420 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 205 -Loss:  2928.7456 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 206 -Loss:  2867.4697 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 207 -Loss:  2920.9653 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 208 -Loss:  3793.7107 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch 209 -Loss:  2954.7402 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 210 -Loss:  3227.0195 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 211 -Loss:  2872.3247 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 212 -Loss:  2855.9246 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 213 -Loss:  2691.5491 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 214 -Loss:  2923.2388 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 215 -Loss:  3059.1204 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 216 -Loss:  3250.9727 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 217 -Loss:  2921.0913 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 218 -Loss:  2667.7593 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 219 -Loss:  2320.1528 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 220 -Loss:  3832.3242 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 221 -Loss:  2536.8926 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 222 -Loss:  3376.4226 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 223 -Loss:  2390.3599 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 224 -Loss:  3051.2485 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 225 -Loss:  2966.8740 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 226 -Loss:  3142.9043 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 227 -Loss:  3214.7690 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 228 -Loss:  3259.1511 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 229 -Loss:  3033.5256 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 230 -Loss:  3128.6584 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 231 -Loss:  2659.5605 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 232 -Loss:  2379.3372 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 233 -Loss:  2245.5693 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 234 -Loss:  2481.0911 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 235 -Loss:  2495.7146 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 236 -Loss:  2179.9250 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 237 -Loss:  2678.3904 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 238 -Loss:  2782.8938 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 239 -Loss:  3161.1523 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 240 -Loss:  3420.8760 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 241 -Loss:  2422.7585 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 242 -Loss:  3306.7092 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 243 -Loss:  2445.5649 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 244 -Loss:  2833.7554 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 245 -Loss:  2500.9434 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 246 -Loss:  3134.6760 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 247 -Loss:  2323.5605 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 248 -Loss:  3053.2822 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 249 -Loss:  2274.6914 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 250 -Loss:  2571.9312 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 251 -Loss:  2488.7935 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 252 -Loss:  2524.7993 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 253 -Loss:  2386.9597 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 254 -Loss:  2471.6284 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 255 -Loss:  2213.8247 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 256 -Loss:  1591.5189 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 257 -Loss:  2740.7808 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 258 -Loss:  2616.7104 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 259 -Loss:  2474.2407 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 260 -Loss:  2274.9883 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 261 -Loss:  2373.5925 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 262 -Loss:  2205.8840 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 263 -Loss:  2542.9065 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 264 -Loss:  2522.8264 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 265 -Loss:  2265.3438 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 266 -Loss:  2565.0859 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 267 -Loss:  2618.8940 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 268 -Loss:  2427.1848 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 269 -Loss:  2537.3979 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 270 -Loss:  2779.2363 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 271 -Loss:  1880.3016 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 272 -Loss:  2112.7334 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 273 -Loss:  2270.3882 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 274 -Loss:  3022.2175 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 275 -Loss:  2755.2700 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 276 -Loss:  2332.1033 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 277 -Loss:  2472.3330 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 278 -Loss:  2324.6450 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 279 -Loss:  2866.1738 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 280 -Loss:  2304.9902 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 281 -Loss:  2156.0352 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 282 -Loss:  2450.6816 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 283 -Loss:  2204.7869 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 284 -Loss:  1690.2123 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 285 -Loss:  1973.8177 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 286 -Loss:  2377.4844 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 287 -Loss:  2564.6685 Validation Accuracy: 0.570312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5ff0f3be3dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 keep_prob: dropout})\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Calculate batch loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\udacity\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# Note: that to get access to tf_activation, the session should be interactive which can be achieved with the following commands.\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.as_default()\n",
    "\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and    max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it maybe having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
